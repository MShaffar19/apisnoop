#+NAME: META
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(b) | TADA(d)
#+INCLUDE: "config.org"
#+TITLE: APIsnoop
#+AUTHOR: ii team
#+DATE: 15 August 2019
#+ARCHIVE: ~/ii/apisnoop_v3/org/archive/meta.archive.org::
#+PROPERTY: header-args:sql-mode+ :results silent

* Purpose
* Raw Swaggers Table, and Helper Functions
  :PROPERTIES:
  :header-args:sql-mode+: :results silent
  :END:
  These are jsonb indexed loads of swagger.json from github k8s commits or releases.
  Eventually these will also be populated on the fly when running within/against a cluster.
** 100: API Swagger Table
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/100_table_api_swagger.up.sql
  :END:
*** Create Table
#+NAME: api_swagger
#+BEGIN_SRC sql-mode
CREATE TABLE api_swagger (
    id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    ingested_at timestamp DEFAULT CURRENT_TIMESTAMP,
    -- version text NOT NULL,
    -- definition_id text NOT NULL,
    data jsonb NOT NULL
);
#+END_SRC
*** Index Table
#+NAME: general index the raw_swagger
#+BEGIN_SRC sql-mode
  CREATE INDEX idx_swagger_jsonb_ops ON api_swagger
    USING GIN (data jsonb_ops);
  CREATE INDEX idx_swagger_jsonb_path_ops ON api_swagger
    USING GIN (data jsonb_path_ops);
#+END_SRC
** 120: Function for load swagger via curl
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/120_function_load_swagger_via_curl.up.sql
  :END:

#+NAME: load_swagger_via_curl.py
#+BEGIN_SRC python :eval never :exports none
  # should probably sanitize branch_or_tag
  try:
      from string import Template
      sql = Template("copy api_swagger (data) FROM PROGRAM '$curl' (DELIMITER e'\x02', FORMAT 'csv', QUOTE e'\x01');").substitute(
      curl =  ''.join([
          'curl https://raw.githubusercontent.com/kubernetes/kubernetes/',
          branch_or_tag,
          '/api/openapi-spec/swagger.json | jq -c .'])
      )
      rv = plpy.execute(sql)
      return "it worked"
  except:
      return "something went wrong"
#+END_SRC

#+NAME: load_swagger_via_curl.sql
#+BEGIN_SRC sql-mode :noweb yes
  set role dba;
  CREATE OR REPLACE FUNCTION load_swagger_via_curl(branch_or_tag text)
  RETURNS text AS $$
  <<load_swagger_via_curl.py>>
  $$ LANGUAGE plpython3u ;
  reset role;
#+END_SRC

** 130: Populate Swaggers Up
  :PROPERTIES:
  :header-args:sql-mode+: :results silent
  :END:
#+NAME: reload swaggers for particluar releases
#+BEGIN_SRC sql-mode
  delete from api_swagger;
  select * from load_swagger_via_curl('master');
  -- select * from load_swagger_via_curl('release-1.15');
  -- select * from load_swagger_via_curl('release-1.14');
  -- select * from load_swagger_via_curl('release-1.13');
  -- select * from load_swagger_via_curl('release-1.12');
  -- select * from load_swagger_via_curl('release-1.11');
  -- select * from load_swagger_via_curl('release-1.10');
#+END_SRC
* Operation Views
  :PROPERTIES:
  :header-args:sql-mode+: :results silent
  :END:
  These are views derived from the 'data->path' section of our api_swagger table.
** 140: api_operation_material view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/140_view_api_operation_material.up.sql
  :END:
  We can track this, but it won't show up in Hasura as it does not support materialized views yet.  We can still use it to create _other_ views hasura can see though.
*** Define regex_from_path function
#+NAME: regex_from_path.py
#+BEGIN_SRC python :eval never :export none
  import re
  if path is None:
    return None
  K8S_PATH_VARIABLE_PATTERN = re.compile("{(path)}$")
  VARIABLE_PATTERN = re.compile("{([^}]+)}")
  path_regex = K8S_PATH_VARIABLE_PATTERN.sub("(.*)", path).rstrip('/')
  path_regex = VARIABLE_PATTERN.sub("([^/]*)", path_regex).rstrip('/')
  if not path_regex.endswith(")") and not path_regex.endswith("?"): 
      path_regex += "([^/]*)"
  if path_regex.endswith("proxy"): 
      path_regex += "/?$"
  else:
      path_regex += "$"
  return path_regex
#+END_SRC

#+NAME: regex_from_path.sql
#+BEGIN_SRC sql-mode :noweb yes 
  set role dba;
  CREATE OR REPLACE FUNCTION regex_from_path(path text)
  RETURNS text AS $$
  <<regex_from_path.py>>
  $$ LANGUAGE plpython3u ;
  reset role;
#+END_SRC

*** Create
#+NAME: api_operation_material
#+BEGIN_SRC sql-mode 
  CREATE MATERIALIZED VIEW "public"."api_operation_material" AS 
    SELECT api_swagger.id AS raw_swagger_id,
           paths.key AS path,
           regex_from_path(paths.key) as regex,
           d.key AS http_method,
           (d.value ->> 'x-kubernetes-action'::text) AS k8s_action,
           (d.value ->> 'operationId'::text) AS operation_id,
           ((d.value -> 'x-kubernetes-group-version-kind'::text) ->> 'group'::text) AS k8s_group,
           ((d.value -> 'x-kubernetes-group-version-kind'::text) ->> 'version'::text) AS k8s_version,
           ((d.value -> 'x-kubernetes-group-version-kind'::text) ->> 'kind'::text) AS k8s_kind,
           (d.value ->> 'description'::text) AS description,
           (d.value -> 'consumes'::text)::jsonb AS consumes,
           (d.value -> 'responses'::text)::jsonb AS responses,
           (d.value -> 'parameters'::text)::jsonb AS parameters,
           (lower((d.value ->> 'description'::text)) ~~ '%deprecated%'::text) AS deprecated,
           split_part((cat_tag.value ->> 0), '_'::text, 1) AS category,
           string_agg(btrim((jsonstring.value)::text, '"'::text), ', '::text) AS tags,
           string_agg(btrim((schemestring.value)::text, '"'::text), ', '::text) AS schemes,
           CASE
            WHEN (d.value ->> 'x-kubernetes-action'::text) = 'get' THEN ARRAY ['get']
            WHEN (d.value ->> 'x-kubernetes-action'::text) =  'list' THEN ARRAY [ 'list' ]
            WHEN (d.value ->> 'x-kubernetes-action'::text) = 'proxy' THEN ARRAY [ 'proxy' ]
            WHEN (d.value ->> 'x-kubernetes-action'::text) = 'deletecollection' THEN ARRAY [ 'deletecollection' ]
            WHEN (d.value ->> 'x-kubernetes-action'::text) = 'watch' THEN ARRAY [ 'watch' ]
            WHEN (d.value ->> 'x-kubernetes-action'::text) = 'post' THEN ARRAY [ 'post', 'create' ]
            WHEN (d.value ->> 'x-kubernetes-action'::text) =  'put' THEN ARRAY [ 'put', 'update' ]
            WHEN (d.value ->> 'x-kubernetes-action'::text) = 'patch' THEN ARRAY [ 'patch' ]
            WHEN (d.value ->> 'x-kubernetes-action'::text) = 'connect' THEN ARRAY [ 'connect' ]
           ELSE NULL
             END as event_verb
      FROM api_swagger
      , jsonb_each((api_swagger.data -> 'paths'::text)) paths(key, value)
      , jsonb_each(paths.value) d(key, value)
      , jsonb_array_elements((d.value -> 'tags'::text)) cat_tag(value)
      , jsonb_array_elements((d.value -> 'tags'::text)) jsonstring(value)
      , jsonb_array_elements((d.value -> 'schemes'::text)) schemestring(value)
     GROUP BY api_swagger.id, paths.key, d.key, d.value, cat_tag.value
     ORDER BY paths.key;
#+END_SRC
*** Index
#+NAME: index the api_operation_material
#+BEGIN_SRC sql-mode :tangle ../apps/hasura/migrations/260_view_api_operation_material.up.sql :results silent
    CREATE INDEX api_operation_materialized_event_verb  ON api_operation_material            (event_verb);
    CREATE INDEX api_operation_materialized_k8s_action  ON api_operation_material            (k8s_action);
    CREATE INDEX api_operation_materialized_k8s_group   ON api_operation_material            (k8s_group);
    CREATE INDEX api_operation_materialized_k8s_version ON api_operation_material            (k8s_version);
    CREATE INDEX api_operation_materialized_k8s_kind    ON api_operation_material            (k8s_kind);
    CREATE INDEX api_operation_materialized_tags        ON api_operation_material            (tags);
    CREATE INDEX api_operation_materialized_schemes     ON api_operation_material            (schemes);
    CREATE INDEX api_operation_materialized_regex_gist  ON api_operation_material USING GIST (regex gist_trgm_ops);
    CREATE INDEX api_operation_materialized_regex_gin   ON api_operation_material USING GIN  (regex gin_trgm_ops);
    CREATE INDEX api_operation_materialized_consumes_ops   ON api_operation_material USING GIN  (consumes jsonb_ops);
    CREATE INDEX api_operation_materialized_consumes_path  ON api_operation_material USING GIN  (consumes jsonb_path_ops);
    CREATE INDEX api_operation_materialized_parameters_ops   ON api_operation_material USING GIN  (parameters jsonb_ops);
    CREATE INDEX api_operation_materialized_parameters_path  ON api_operation_material USING GIN  (parameters jsonb_path_ops);
    CREATE INDEX api_operation_materialized_responses_ops   ON api_operation_material USING GIN  (responses jsonb_ops);
    CREATE INDEX api_operation_materialized_responses_path  ON api_operation_material USING GIN  (responses jsonb_path_ops);
#+END_SRC
** 150: api_operation view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/150_view_api_operation.up.sql
  :END:
   This grabs the 'paths' section of our swagger.json, where each path contains operation Id, tags, schemes, etc.
*** Create View
#+NAME: api_operation view
#+BEGIN_SRC sql-mode 
  CREATE OR REPLACE VIEW "public"."api_operation" AS 
    SELECT * from api_operation_material;
#+END_SRC
** 160: api_operation_parameter view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/160_view_api_operation_parameter.up.sql
  :END:
Using our api_operation view, look into the parameters field in each one.     
*** Create
 #+NAME: api_operation_parameter view
 #+BEGIN_SRC sql-mode
   CREATE OR REPLACE VIEW "public"."api_operation_parameter" AS 
     SELECT (param.entry ->> 'name'::text) AS name,
            (param.entry ->> 'in'::text) AS "in",
            -- for resource:
            -- if param is body in body, take its $ref from its schema
            -- otherwise, take its type
            replace(
              CASE
              WHEN ((param.entry ->> 'in'::text) = 'body'::text) 
               AND ((param.entry -> 'schema'::text) is not null)
                THEN ((param.entry -> 'schema'::text) ->> '$ref'::text)
              ELSE (param.entry ->> 'type'::text)
              END, '#/definitions/','') AS resource,
            (param.entry ->> 'description'::text) AS description,
            CASE
            WHEN ((param.entry ->> 'required'::text) = 'true') THEN true
            ELSE false
             END AS required,
            CASE
            WHEN ((param.entry ->> 'uniqueItems'::text) = 'true') THEN true
            ELSE false
            END AS unique_items,
            api_operation.raw_swagger_id,
            param.entry as entry,
            api_operation.operation_id
       FROM api_operation
            , jsonb_array_elements(api_operation.parameters) WITH ORDINALITY param(entry, index)
             WHERE api_operation.parameters IS NOT NULL;
 #+END_SRC

** 180: api_operation_response view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/180_view_api_operation_response.up.sql
  :END:
   Similar to parameters, within each of the paths of the swagger.json, there is a responses field.  We are listing the values within this field.
*** Create
 #+NAME: Responses View
 #+BEGIN_SRC sql-mode 
   CREATE OR REPLACE VIEW "public"."api_operation_response" AS 
     SELECT d.key AS code,
            (d.value ->> 'description'::text) AS description,
            replace(
              CASE
              WHEN (((d.value -> 'schema'::text) IS NOT NULL) AND (((d.value -> 'schema'::text) -> 'type'::text) IS NOT NULL))
                THEN ((d.value -> 'schema'::text) ->> 'type'::text)
              WHEN (((d.value -> 'schema'::text) IS NOT NULL) AND (((d.value -> 'schema'::text) -> '$ref'::text) IS NOT NULL))
                THEN ((d.value -> 'schema'::text) ->> '$ref'::text)
              ELSE NULL::text
              END, '#/definitions/','') AS resource,
              api_operation.operation_id,
              api_operation.raw_swagger_id
       FROM (api_operation
             JOIN LATERAL jsonb_each(api_operation.responses) d(key, value) ON (true))
      ORDER BY (uuid_generate_v1());
 #+END_SRC
** 270: api_operation_parameter_material
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/270_view_api_operation_parameter_material.up.sql
  :END:
*** Create
Using our api_operation_material view, look into the parameters field in each one.     
#+NAME: api_operation_parameter_material view
#+BEGIN_SRC sql-mode
  CREATE MATERIALIZED VIEW "public"."api_operation_parameter_material" AS 
    SELECT (param.entry ->> 'name'::text) AS name,
           (param.entry ->> 'in'::text) AS "in",
           -- for resource:
           -- if param is body in body, take its $ref from its schema
           -- otherwise, take its type
           replace(
             CASE
             WHEN ((param.entry ->> 'in'::text) = 'body'::text) 
              AND ((param.entry -> 'schema'::text) is not null)
               THEN ((param.entry -> 'schema'::text) ->> '$ref'::text)
             ELSE (param.entry ->> 'type'::text)
             END, '#/definitions/','') AS resource,
           (param.entry ->> 'description'::text) AS description,
           CASE
           WHEN ((param.entry ->> 'required'::text) = 'true') THEN true
           ELSE false
            END AS required,
           CASE
           WHEN ((param.entry ->> 'uniqueItems'::text) = 'true') THEN true
           ELSE false
           END AS unique_items,
           api_operation_material.raw_swagger_id,
           param.entry as entry,
           api_operation_material.operation_id
      FROM api_operation_material
           , jsonb_array_elements(api_operation_material.parameters) WITH ORDINALITY param(entry, index)
            WHERE api_operation_material.parameters IS NOT NULL;
#+END_SRC
*** Index
#+NAME: index the api_operation_material
#+BEGIN_SRC sql-mode
    CREATE UNIQUE INDEX                                  ON api_operation_parameter_material(raw_swagger_id, operation_id, name);
    CREATE INDEX api_parameters_materialized_resource    ON api_operation_parameter_material            (resource);
    CREATE INDEX api_parameters_materialized_entry       ON api_operation_parameter_material            (entry);
#+END_SRC

* Resource Views
  :PROPERTIES:
  :header-args:sql-mode+: :results silent
  :END:
  These are views derived from the 'data->definitions' in our api_swagger table
** 190: api_schema view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/190_view_api_schema.up.sql
  :END:
*** Create
 #+NAME: api_schema view
 #+BEGIN_SRC sql-mode 
   CREATE VIEW "public"."api_schema" AS 
    SELECT 
       api_swagger.id AS raw_swagger_id,
       d.key AS name,
       (d.value ->> 'type'::text) AS resource_type,
       (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'group'::text) AS k8s_group,
       (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'version'::text) AS k8s_version,
       (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'kind'::text) AS k8s_kind,
       string_agg(btrim((reqstring.value)::text, '"'::text), ', '::text) AS required_params,
       (d.value ->> 'required'::text) as required_params_text,
       (d.value -> 'properties'::text) AS properties,
       d.value
      FROM api_swagger
        , jsonb_each((api_swagger.data -> 'definitions'::text)) d(key, value)
        , jsonb_array_elements((d.value -> 'required'::text)) reqstring(value)
      GROUP BY api_swagger.id, d.key, d.value;

 #+END_SRC
** 200: api_schema_field view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/200_view_api_schema_field.up.sql
  :END:
*** Create
#+NAME: api_schema_field view
#+BEGIN_SRC sql-mode 
  CREATE VIEW "public"."api_schema_field" AS 
    SELECT api_schema.name as api_resource_name,
           api_schema.raw_swagger_id,
           d.key AS resource_field,
           CASE
           WHEN ((d.value ->> 'type'::text) IS NULL) THEN 'subtype'::text
           ELSE (d.value ->> 'type'::text)
             END AS param_type,
           replace(
             CASE
             WHEN ((d.value ->> 'type'::text) = 'string'::text) THEN 'string'::text
             WHEN ((d.value ->> 'type'::text) IS NULL) THEN (d.value ->> '$ref'::text)
             WHEN ((d.value ->> 'type'::text) = 'array'::text)
              AND ((d.value -> 'items'::text) ->> 'type'::text) IS NULL
               THEN ((d.value -> 'items'::text) ->> '$ref'::text)
             WHEN ((d.value ->> 'type'::text) = 'array'::text)
              AND ((d.value -> 'items'::text) ->> '$ref'::text) IS NULL
               THEN ((d.value -> 'items'::text) ->> 'type'::text)
             ELSE 'integer'::text
             END, '#/definitions/','') AS param_kind,
           (d.value ->> 'description'::text) AS description,
           (d.value ->> 'format'::text) AS format,
           (d.value ->> 'x-kubernetes-patch-merge-key'::text) AS merge_key,
           (d.value ->> 'x-kubernetes-patch-strategy'::text) AS patch_strategy,
           d.value
      FROM (api_schema
            JOIN LATERAL jsonb_each(api_schema.properties) d(key, value) ON (true));
#+END_SRC
* OpenAPI Over Views
  :PROPERTIES:
  :header-args:sql-mode+: :results silent
  :END:
  This combines fields from the majority fo the above views to create one uber view.
** 210: over view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/210_view_over.up.sql
  :END:
*** Create
 #+NAME: over view
 #+BEGIN_SRC sql-mode
   CREATE OR REPLACE VIEW "public"."over" AS
     SELECT
       op.name as param_name,
       op.required as param_required,
       op.description as param_description,
       o.operation_id,
       op.resource,
       r.name as resource_name,
       r.k8s_group,
       r.k8s_version,
       r.k8s_kind,
       rf.resource_field,
       rf.param_type,
       rf.param_kind,
       rf.description,
       rf.format,
       rf.merge_key,
       rf.patch_strategy
       FROM
           api_operation_parameter op
           JOIN api_operation o ON (
             o.raw_swagger_id = op.raw_swagger_id
             AND
             o.operation_id = op.operation_id
           )
           LEFT JOIN api_schema r ON (
             op.resource = r.name
             AND
             op.raw_swagger_id = r.raw_swagger_id
             )
           LEFT JOIN api_schema_field rf ON (
             rf.api_resource_name = r.name
             AND
             rf.raw_swagger_id = r.raw_swagger_id
           )
      ORDER BY op.name;
 #+END_SRC
* Raw Audit Events Table, and helper functions
  :PROPERTIES:
  :header-args:sql-mode+: :results silent
  :END:
** 220: raw_audit_event Table
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/220_table_raw_audit_event.up.sql
  :END:
*** Create
#+NAME: raw_audit_event
#+BEGIN_SRC sql-mode
  CREATE UNLOGGED TABLE raw_audit_event (
    -- id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    -- ingested_at timestamp DEFAULT CURRENT_TIMESTAMP,
    bucket text,
    job text,
    audit_id text NOT NULL,
    stage text NOT NULL,
    event_verb text NOT NULL,
    request_uri text NOT NULL,
    operation_id text,
    data jsonb NOT NULL
  );
#+END_SRC
*** Index
#+NAME: index the raw_audit_event
#+BEGIN_SRC sql-mode
-- CREATE INDEX idx_audit_event_primary          ON raw_audit_event (bucket, job, audit_id, stage);
-- ALTER TABLE raw_audit_event add primary key using index idx_audit_event_primary;
CREATE INDEX idx_audit_event_jsonb_ops        ON raw_audit_event USING GIN (data jsonb_ops);
CREATE INDEX idx_audit_event_jsonb_path_jobs  ON raw_audit_event USING GIN (data jsonb_path_ops);
#+END_SRC
** 245: load_audit_event Function
  :PROPERTIES:
  :header-args:sql-mode+:  :tangle ../apps/hasura/migrations/245_function_load_audit_event.up.sql
  :END:
*** Python Code
#+NAME: load_audit_events.py
#+BEGIN_SRC python :noweb yes :exports none
  #:tangle ../apps/hasura/migrations/245_function_load_audit_events.up.sql :results silent
  #!/usr/bin/env python3
  from urllib.request import urlopen, urlretrieve
  import os
  import re
  from bs4 import BeautifulSoup
  import subprocess
  import time
  import glob
  from tempfile import mkdtemp
  from string import Template


  def get_html(url):
      html = urlopen(url).read()
      soup = BeautifulSoup(html, 'html.parser')
      return soup


  def download_url_to_path(url, local_path):
      local_dir = os.path.dirname(local_path)
      if not os.path.isdir(local_dir):
          os.makedirs(local_dir)
      if not os.path.isfile(local_path):
          process = subprocess.Popen(['wget', '-q', url, '-O', local_path])
          downloads[local_path] = process

  # this global dict is used to track our wget subprocesses
  # wget was used because the files can get to several halfa gig
  downloads = {}
  def load_audit_events(bucket,job):
      bucket_url = f'https://storage.googleapis.com/kubernetes-jenkins/logs/{bucket}/{job}/'
      artifacts_url = f'https://gcsweb.k8s.io/gcs/kubernetes-jenkins/logs/{bucket}/{job}/artifacts'
      job_metadata_files = [
          'finished.json',
          'artifacts/metadata.json',
          'artifacts/junit_01.xml',
          'build-log.txt'
      ]
      download_path = mkdtemp( dir='/tmp', prefix=f'apisnoop-{bucket}-{job}' ) + '/'
      combined_log_file = download_path + 'audit.log'

      # meta data to download
      for jobfile in job_metadata_files:
          download_url_to_path( bucket_url + jobfile,
                                download_path + jobfile )

      # Use soup to grab url of each of audit.log.* (some end in .gz)
      soup = get_html(artifacts_url)
      master_link = soup.find(href=re.compile("master"))
      master_soup = get_html(
          "https://gcsweb.k8s.io" + master_link['href'])
      log_links = master_soup.find_all(
          href=re.compile("audit.log"))

      # download all logs
      for link in log_links:
          log_url = link['href']
          log_file = download_path + os.path.basename(log_url)
          download_url_to_path( log_url, log_file)

      # Our Downloader uses subprocess of curl for speed
      for download in downloads.keys():
          # Sleep for 5 seconds and check for next download
          while downloads[download].poll() is None:
              time.sleep(5)
              # print("Still downloading: " + download)
          # print("Downloaded: " + download)

      # Loop through the files, (z)cat them into a combined audit.log
      with open(combined_log_file, 'ab') as log:
          for logfile in sorted(
                  glob.glob(download_path + '*kube-apiserver-audit*'), reverse=True):
              if logfile.endswith('z'):
                  subprocess.run(['zcat', logfile], stdout=log, check=True)
              else:
                  subprocess.run(['cat', logfile], stdout=log, check=True)
      # Load the resulting combined audit.log directly into raw_audit_event
      try:
          # for some reason tangling isn't working to reference this SQL block
          sql = Template("""
  CREATE TEMPORARY TABLE raw_audit_event_import (data jsonb not null) ;
  COPY raw_audit_event_import (data)
  FROM '${audit_logfile}' (DELIMITER e'\x02', FORMAT 'csv', QUOTE e'\x01');

  INSERT INTO raw_audit_event(bucket, job,
                               audit_id, stage,
                               event_verb, request_uri,
                               operation_id,
                               data)
  SELECT '${bucket}', '${job}',
         (raw.data ->> 'auditID'), (raw.data ->> 'stage'),
         (raw.data ->> 'verb'), (raw.data ->> 'requestURI'),
         ops.operation_id,
         raw.data 
    FROM raw_audit_event_import raw
           -- FIXME: this join is necesary, but expensive
           -- https://github.com/cncf/apisnoopregexp is an alterative approach
           LEFT JOIN api_operation_material ops ON
             ops.raw_swagger_id = 1
               AND raw.data ->> 'verb' = ANY(ops.event_verb)
               AND raw.data ->> 'requestURI' ~ ops.regex;
          """).substitute(
              audit_logfile = combined_log_file,
              bucket = bucket,
              job = job
          )
          with open(download_path + 'load.sql', 'w') as sqlfile:
            sqlfile.write(sql)
          rv = plpy.execute(sql)
          return "it worked"
      except plpy.SPIError:
          return "something went wrong with plpy"
      except:
          return "something unknown went wrong"
  if __name__ == "__main__":
      load_audit_events('ci-kubernetes-e2e-gci-gce','1134962072287711234')
  else:
      load_audit_events(bucket,job)
#+END_SRC
*** Create
#+NAME: load_audit_events.sql
#+BEGIN_SRC sql-mode :noweb yes
  set role dba;
  CREATE OR REPLACE FUNCTION load_audit_events(bucket text, job text)
  RETURNS text AS $$
  <<load_audit_events.py>>
  $$ LANGUAGE plpython3u ;
  reset role;
#+END_SRC
** 250: populate_audit_events.up.sql
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/250_populate_audit_events.up.sql
  :END:
*** Call load_audit_events()
#+NAME: reload sample audit event
#+BEGIN_SRC sql-mode :noweb yes
  select * from load_audit_events('ci-kubernetes-e2e-gci-gce','1134962072287711234');
#+END_SRC
* Audit Events View
  :PROPERTIES:
  :header-args:sql-mode+: :results silent
  :END:
   We'll need to figure out which is the correct audit events here
** 225: Audit Events View
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/225_view_audit_event.up.sql
  :END:
*** Create
    #+NAME: view audit_event
    #+BEGIN_SRC sql-mode
      CREATE OR REPLACE VIEW "public"."audit_event" AS
        SELECT (raw.data ->> 'auditID') as audit_id,
               raw.bucket,
               raw.job,
               raw.data ->> 'level' as event_level,
               raw.data ->> 'stage' as event_stage,
               raw.operation_id,
               -- yops.operation_id,
               -- ops.k8s_action,
               -- ops.http_method,
               -- event_verb_to_http_method(raw.data ->> 'verb') AS operation_verb,
               -- ops.path as op_path,
               raw.data ->> 'verb' as event_verb,
               raw.data ->> 'apiVersion' as api_version,
               raw.data ->> 'requestURI' as request_uri,
               -- Always "Event"
               -- raw.data ->> 'kind' as kind,
               raw.data ->> 'userAgent' as useragent,
               raw.data -> 'user' as event_user,
               raw.data #>> '{objectRef,namespace}' as object_namespace,
               raw.data #>> '{objectRef,resource}' as object_type,
               raw.data #>> '{objectRef,apiGroup}' as object_group,
               raw.data #>> '{objectRef,apiVersion}' as object_ver,
               raw.data -> 'sourceIPs' as source_ips,
               raw.data -> 'annotations' as annotations,
               raw.data -> 'requestObject' as request_object,
               raw.data -> 'responseObject' as response_object,
               raw.data -> 'responseStatus' as response_status,
               raw.data ->> 'stageTimestamp' as stage_timestamp,
               raw.data ->> 'requestReceivedTimestamp' as request_received_timestamp,
               raw.data as data
        FROM raw_audit_event raw;
    #+END_SRC
* PodSpec Field Coverage
  :PROPERTIES:
  :header-args:sql-mode+: :results silent
  :END:
** 300: PodSpec Materialized View
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/300_podspec_field_coverage_material.up.sql
  :END:
#+NAME: view podspec_field_coverage_material
#+BEGIN_SRC sql-mode
  CREATE MATERIALIZED VIEW "public"."podspec_field_coverage_material" AS 
  SELECT DISTINCT
    audit_event.operation_id,
    jsonb_object_keys(audit_event.request_object -> 'spec'::text) AS podspec_field,
    count(event_field.event_field) AS hits,
    split_part(audit_event.useragent, '--', 2) as test,
    split_part(audit_event.useragent, '--', 1) as useragent
    FROM audit_event,
         LATERAL
           jsonb_object_keys(audit_event.request_object -> 'spec'::text)
           event_field(event_field)
   WHERE (audit_event.request_object ->> 'kind'::text) = 'Pod'::text
     AND audit_event.operation_id !~~ '%alpha%'::text
     AND audit_event.operation_id !~~ '%beta%'::text
   GROUP BY operation_id, podspec_field, useragent
  UNION
  SELECT DISTINCT
    audit_event.operation_id,
    jsonb_object_keys(audit_event.request_object -> 'template' -> 'spec'::text) AS podspec_field,
    count(event_field.event_field) AS hits,
    split_part(audit_event.useragent, '--', 2) as test,
    split_part(audit_event.useragent, '--', 1) as useragent
    FROM audit_event,
         LATERAL
           jsonb_object_keys(audit_event.request_object -> 'template' -> 'spec'::text)
           event_field(event_field)
   WHERE (audit_event.request_object ->> 'kind'::text) = 'PodTemplate'::text
     AND audit_event.operation_id !~~ '%alpha%'::text
     AND audit_event.operation_id !~~ '%beta%'::text
   GROUP BY operation_id, podspec_field, useragent
  UNION
  SELECT DISTINCT
    audit_event.operation_id,
    jsonb_object_keys(audit_event.request_object -> 'spec' -> 'template' -> 'spec'::text) AS podspec_field,
    count(event_field.event_field) AS hits,
    split_part(audit_event.useragent, '--', 2) as test,
    split_part(audit_event.useragent, '--', 1) as useragent
    FROM audit_event,
         LATERAL
           jsonb_object_keys(audit_event.request_object -> 'spec' -> 'template' -> 'spec'::text)
           event_field(event_field)
   WHERE (audit_event.request_object->>'kind' = 'DaemonSet'
     OR  audit_event.request_object->>'kind' = 'Deployment'
     OR  audit_event.request_object->>'kind' = 'ReplicationController'
     OR  audit_event.request_object->>'kind' = 'StatefulSet'
     OR  audit_event.request_object->>'kind' = 'Job'
     OR  audit_event.request_object->>'kind' = 'ReplicaSet')
     AND audit_event.operation_id !~~ '%alpha%'::text
     AND audit_event.operation_id !~~ '%beta%'::text
   GROUP BY operation_id, podspec_field, useragent;
#+END_SRC
** 301: PodSpec Field Coverage View
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/301_view_podspec_field_coverage.up.sql
  :END:
#+NAME: view podspec_field_coverage
#+BEGIN_SRC sql-mode
create view podspec_field_coverage as select * from podspec_field_coverage_material;
#+END_SRC
** 302: PodSpec Field Summary View
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/302_view_podspec_field_summary.up.sql
  :END:
#+NAME: view podspec_field_summary
#+BEGIN_SRC sql-mode
  create view podspec_field_summary as
  select distinct resource_field as podspec_field,
                  0 as other_hits,
                  0 as e2e_hits,
                  0 as conf_hits
    from api_schema_field
   where api_resource_name like '%PodSpec%'
   UNION
  select
    podspec_field,
    sum(hits) as other_hits,
    0 as e2e_hits,
    0 as conf_hits
    from podspec_field_coverage
   where useragent not like 'e2e.test%'
   group by podspec_field
   UNION
  select
    podspec_field,
    0 as other_hits,
    sum(hits) as e2e_hits,
    0 as conf_hits
    from podspec_field_coverage
   where useragent like 'e2e.test%'
     and test not like '%Conformance%'
   group by podspec_field
   UNION
  select
    podspec_field,
    0 as other_hits,
    0 as e2e_hits,
    sum(hits) as conf_hits
    from podspec_field_coverage
   where useragent like 'e2e.test%'
     and test like '%Conformance%'
   group by podspec_field;
#+END_SRC
** 304: PodSpec Field Hits
#+NAME: podspec_field_hits
#+BEGIN_SRC sql-mode
select distinct podspec_field,
      sum(other_hits) as other_hits,
      sum(e2e_hits) as e2e_hits,
      sum(conf_hits) as conf_hits
from podspec_field_summary
group by podspec_field
order by conf_hits, e2e_hits, other_hits;
#+END_SRC

* Hasura
** Tracking Tables
  :PROPERTIES:
  :header-args:yaml+: :tangle ../apps/hasura/migrations/999_tracking.up.yaml
  :END:
*** api_swagger
#+NAME: track api_swagger
#+BEGIN_SRC yaml
- type: track_table
  args:
    schema: public
    name: api_swagger
#+END_SRC
*** api_operation
#+NAME: track api_operation
#+BEGIN_SRC yaml
- type: track_table
  args:
    schema: public
    name: api_operation
#+END_SRC

*** api_operation_parameter

 #+NAME: track api_operation_parameter
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: api_operation_parameter
 #+END_SRC
*** api_operation_response
 #+NAME: track api_operation_response
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: api_operation_response
 #+END_SRC

*** api_schema
 #+NAME: track api_schema
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: api_schema
 #+END_SRC

*** api_schema_field
#+NAME: track api_schema_field
#+BEGIN_SRC yaml
- type: track_table
  args:
    schema: public
    name: api_schema_field
#+END_SRC

*** over
 #+NAME: track over
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: over
 #+END_SRC
*** raw_audit_event
#+NAME: track raw_audit_event
#+BEGIN_SRC yaml
- type: track_table
  args:
    schema: public
    name: raw_audit_event
#+END_SRC
*** audit_event
 #+NAME: track audit_event
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: audit_event
 #+END_SRC

** docker-compose.yml
#+NAME: hasura docker-compose
#+BEGIN_SRC yaml :tangle ../apps/hasura/docker-compose.yaml
  # hasura/docker-compose.yaml
  version: "3.7"

  services:
   hasura:
      #image: hasura/graphql-engine:v1.0.0-beta.3
      # append '.cli-migrations' to auto run 'hasura migrations apply'
      container_name: "${USER}-hasura"
      image: hasura/graphql-engine:v1.0.0-beta.4.cli-migrations
      restart: always
      networks:
        - web
      environment:
        # Should try and set database be read only for public
        #- HASURA_GRAPHQL_DATABASE_URL=postgres://non-priv-user@172.17.0.1:5432/database-name
        #- HASURA_GRAPHQL_DATABASE_URL=postgres://non-priv-user@172.17.0.1:5432/$OUTER-USER
        # https://docs.docker.com/compose/compose-file/#variable-substitution
        # https://docs.docker.com/compose/env-file/
        - "HASURA_GRAPHQL_DATABASE_URL=postgres://${USER}@172.17.0.1:5432/${USER}"
        - HASURA_GRAPHQL_ENABLE_CONSOLE=true
      volumes:
        - ./migrations:/hasura-migrations
      expose:
        - "8080"
      labels:
        - "traefik.docker.network=web"
        - "traefik.enable=true"
        - "traefik.basic.port=8080"
        - "traefik.basic.protocol=http"
        - "traefik.basic.frontend.rule=Host:${USER}-hasura.sharing.io"
  #volumes:
  #  migrations:
  networks:
    web:
      external: true
#+END_SRC
* FOOTNOTES
** regex matcher
*** apisnoopregex
#+BEGIN_SRC  shell
  cd ~hh/ii/apisnoopregexp
  make rmatch ; CONN="host=172.17.0.1 user=$USER dbname=$USER password=''" ./rmatch
#+END_SRC

*** TODO bulk updater in sqlpseudo code
#+BEGIN_SRC sql-mode
  -- find all api_operation.regex
  select distinct regex from api_operation;
  -- find all audit_event.request_uri
  select distinct request_uri from raw_audit_event where operation_id is null;
  -- find all event_verbs for each audit_event.request_uri
  select distinct event_verb from raw_audit_event where operation_id is null and request_uri = $1;
  -- find the operation_id by searching the api_aperations just once per verb/uri combo
  -- update the entire set of uri+verb to the right operation at once
  update raw_audit_event set operation_id = $1 where request_uri = $2 and event_verb = $3;
#+END_SRC
*** TODO on injection
#+BEGIN_SRC sql-mode
  -- look up url+verb => operation in url_verb_operation table
  -- query once to retrieve
  -- update url_verb_operation table
#+END_SRC

** Iteration Loop
*** listing tables
 #+BEGIN_SRC sql-mode
   \conninfo
   \d+
 #+END_SRC

 #+RESULTS:
 #+begin_src sql-mode
 You are connected to database "hh" as user "hh" on host "172.17.0.1" at port "5432".
 SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)
 #+end_src

*** dropping all data
 #+NAME: do not run
 #+BEGIN_SRC sql-mode
   drop table api_swagger cascade;
   drop table raw_audit_event cascade;
   drop schema hdb_catalog cascade;
   drop schema hdb_views cascade;
 #+END_SRC

 #+RESULTS: do not run
 #+begin_src sql-mode
 ERROR:  table "api_swagger" does not exist
 ERROR:  table "raw_audit_event" does not exist
 ERROR:  schema "hdb_catalog" does not exist
 ERROR:  schema "hdb_views" does not exist
 #+end_src
*** Restart Hasura
    #+NAME: iteration loop
    #+BEGIN_SRC tmate 
      #:dir ~/ii/apisnoop_v3/apps/hasura
      
      
      
      cd ~/ii/apisnoop_v3/apps
      echo create a $USER-hasura dir and link back to ../hasura/*
      mkdir -p $USER-hasura && cd $USER-hasura && ln -sf ../hasura/* .
      echo docker down, drop sql, rm migrations
      # For tangling via cli:
      SOCKETFILE=$(ls /tmp/emacs$(id -u)/server* | head -1)
      docker-compose down -t 0 \
       && docker-compose rm \
       && echo 'drop table api_swagger cascade; \
                drop table raw_audit_event cascade; \
                drop schema hdb_views cascade; \
                drop schema hdb_catalog cascade;' | psql \
       &&  rm migrations/* \
       && emacsclient -s $SOCKETFILE -e "(org-babel-tangle-file \"~/ii/apisnoop_v3/org/meta.org\")" \
       && docker-compose up
       # cd ~hh/ii/apisnoopregexp
       # make rmatch ; CONN="host=172.17.0.1 user=$USER dbname=$USER password=''" ./rmatch
    #+END_SRC

    #+RESULTS: iteration loop
    #+begin_EXAMPLE
    #+end_EXAMPLE

*** setting up the hasura postgresql-permissions
   
 Run the following as the postgres user via psql:
 https://docs.hasura.io/1.0/graphql/manual/deployment/postgres-permissions.html

 #+NAME: hasura-user
 #+BEGIN_SRC shell :eval never-export :results silent
 echo -n $USER
 #+END_SRC

 #+NAME: create database and granting all privs to a user
 #+BEGIN_SRC sql-mode :noweb yes :tangle ../apps/hasura/db_setup.sql
 create database <<hasura-user()>>;
 -- create user myuser with encrypted password 'mypass';
 grant all privileges on database <<hasura-user()>> to <<hasura-user()>>;
 create role dba with superuser noinherit;
 grant dba to <<hasura-user()>>;
 \connect <<hasura-user()>>
 -- we write python functions
 CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
 CREATE EXTENSION IF NOT EXISTS plpython3u;
 CREATE EXTENSION IF NOT EXISTS plsh;
 CREATE EXTENSION IF NOT EXISTS pgcrypto;
 CREATE EXTENSION IF NOT EXISTS pg_trgm;
 CREATE SCHEMA IF NOT EXISTS hdb_catalog;
 CREATE SCHEMA IF NOT EXISTS hdb_views;
 -- make the user an owner of system schemas
 ALTER SCHEMA hdb_catalog OWNER TO <<hasura-user()>>;
 ALTER SCHEMA hdb_views OWNER TO <<hasura-user()>>;
 GRANT SELECT ON ALL TABLES IN SCHEMA information_schema TO <<hasura-user()>>;
 GRANT SELECT ON ALL TABLES IN SCHEMA pg_catalog TO <<hasura-user()>>;
 GRANT USAGE ON SCHEMA public TO <<hasura-user()>>;
 GRANT ALL ON ALL TABLES IN SCHEMA public TO <<hasura-user()>>;
 GRANT ALL ON ALL SEQUENCES IN SCHEMA public TO <<hasura-user()>>;
 GRANT pg_execute_server_program TO <<hasura-user()>>;
 #+END_SRC

 #+RESULTS: create database and granting all privs to a user
 #+begin_src sql-mode
 ERROR:  database "zz" already exists
 WARNING:  no privileges were granted for "zz"
 GRANT
 ERROR:  must be superuser to create superusers
 ERROR:  must be superuser to alter superusers
 SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)
 You are now connected to database "zz" as user "zz".
 NOTICE:  extension "uuid-ossp" already exists, skipping
 CREATE EXTENSION
 NOTICE:  extension "plpython3u" already exists, skipping
 CREATE EXTENSION
 ERROR:  permission denied to create extension "plsh"
 HINT:  Must be superuser to create this extension.
 NOTICE:  extension "pgcrypto" already exists, skipping
 CREATE EXTENSION
 NOTICE:  schema "hdb_catalog" already exists, skipping
 CREATE SCHEMA
 NOTICE:  schema "hdb_views" already exists, skipping
 CREATE SCHEMA
 ALTER SCHEMA
 ALTER SCHEMA
 WARNING:  no privileges were granted for "sql_features"
 WARNING:  no privileges were granted for "sql_implementation_info"
 WARNING:  no privileges were granted for "sql_languages"
 WARNING:  no privileges were granted for "sql_packages"
 WARNING:  no privileges were granted for "sql_parts"
 WARNING:  no privileges were granted for "sql_sizing"
 WARNING:  no privileges were granted for "sql_sizing_profiles"
 WARNING:  no privileges were granted for "collations"
 WARNING:  no privileges were granted for "collation_character_set_applicability"
 WARNING:  no privileges were granted for "column_domain_usage"
 WARNING:  no privileges were granted for "constraint_column_usage"
 WARNING:  no privileges were granted for "domains"
 WARNING:  no privileges were granted for "constraint_table_usage"
 WARNING:  no privileges were granted for "domain_constraints"
 WARNING:  no privileges were granted for "role_column_grants"
 WARNING:  no privileges were granted for "enabled_roles"
 WARNING:  no privileges were granted for "referential_constraints"
 WARNING:  no privileges were granted for "parameters"
 WARNING:  no privileges were granted for "routine_privileges"
 WARNING:  no privileges were granted for "role_routine_grants"
 WARNING:  no privileges were granted for "routines"
 WARNING:  no privileges were granted for "schemata"
 WARNING:  no privileges were granted for "table_constraints"
 WARNING:  no privileges were granted for "sequences"
 WARNING:  no privileges were granted for "triggered_update_columns"
 WARNING:  no privileges were granted for "table_privileges"
 WARNING:  no privileges were granted for "role_table_grants"
 WARNING:  no privileges were granted for "triggers"
 WARNING:  no privileges were granted for "tables"
 WARNING:  no privileges were granted for "transforms"
 WARNING:  no privileges were granted for "role_udt_grants"
 WARNING:  no privileges were granted for "udt_privileges"
 WARNING:  no privileges were granted for "role_usage_grants"
 WARNING:  no privileges were granted for "usage_privileges"
 WARNING:  no privileges were granted for "user_defined_types"
 WARNING:  no privileges were granted for "view_column_usage"
 WARNING:  no privileges were granted for "view_routine_usage"
 WARNING:  no privileges were granted for "view_table_usage"
 WARNING:  no privileges were granted for "views"
 WARNING:  no privileges were granted for "data_type_privileges"
 WARNING:  no privileges were granted for "user_mapping_options"
 WARNING:  no privileges were granted for "element_types"
 WARNING:  no privileges were granted for "_pg_foreign_table_columns"
 WARNING:  no privileges were granted for "column_options"
 WARNING:  no privileges were granted for "_pg_foreign_data_wrappers"
 WARNING:  no privileges were granted for "foreign_data_wrapper_options"
 WARNING:  no privileges were granted for "user_mappings"
 WARNING:  no privileges were granted for "foreign_data_wrappers"
 WARNING:  no privileges were granted for "_pg_foreign_servers"
 WARNING:  no privileges were granted for "foreign_server_options"
 WARNING:  no privileges were granted for "foreign_servers"
 WARNING:  no privileges were granted for "_pg_foreign_tables"
 WARNING:  no privileges were granted for "key_column_usage"
 WARNING:  no privileges were granted for 
 #+end_src

 #+NAME: as posgres admin, setup hasura user and db
 #+BEGIN_SRC tmate :eval never-export
 #+BEGIN_SRC shell  :var SUDO_ASKPASS="/usr/bin/ssh-askpass" :prologue "export SUDO_ASKPASS"
 # :var DISPLAY=":0.0"
 sudo su - postgres -c psql < ~/ii/apisnoop_v3/apps/hasura/db_setup.sql
 #+END_SRC

 #+RESULTS: as posgres admin, setup hasura user and db
 #+begin_EXAMPLE
 #+end_EXAMPLE

** Local Variables
 # Local Variables:
 # noeval: (sql-connect "hasura" (concat "*SQL: postgres:data*"))
 # End:
