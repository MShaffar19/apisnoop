# -*- ii: enabled; -*-
#+TITLE: Migrating to AWS using flux
#+PROPERTY: header-args:tmate+  :session flux
#+PROPERTY: header-args:tmate+  :window deploy
#+PROPERTY: header-args:shell+  :var FLUX_FORWARD_NAMESPACE="flux"
#+PROPERTY: header-args:shell+  :prologue "export FLUX_FORWARD_NAMESPACE"
* cluster - aws / eksctl cli
** install eksctl
   #+begin_src tmate
     brew tap weaveworks/tap
     brew install weaveworks/tap/eksctl
   #+end_src
** eksctl create cluster
*** cluster config
    #+begin_src yaml :tangle eks-cluster.yaml :comments no
      apiVersion: eksctl.io/v1alpha5
      kind: ClusterConfig

      metadata:
        name: apisnoop
        region: ap-southeast-2
        version: '1.15'

      nodeGroups:
        - name: bigs
          instanceType: m5d.24xlarge
          # instanceType: m5.large
          minSize: 1
          desiredCapacity: 2
          maxSize: 3
          volumeSize: 1000
          volumeType: gp2
          # volumeType: io1
          # volumeIOPS: 2000
          iam:
            withAddonPolicies:
              externalDNS: true
              certManager: true
              imageBuilder: true
              ebs: true
              albIngress: true
              cloudWatch: true
          ssh:
            # publicKeyPath: ~/.ssh/id_rsa-4096-20090605-ccc.pub
            publicKey: "ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAgEAsUXHYuDzE6fs2KkCj91qSqpMXyxozb9gDKcx3mlh87hCegevld75gQAhujVYGRgJLsdf7W0/lX81clCRP1FjbaYYrPkVWGR291U6K5rkL9kZqd9dC0h9iCvFTKdKC7sA/uaolFPWav3QFWdEp3geNNuAm/NKSckUs9yGgr1inANQNsHFl0JFzU34D2Kt43rKA0Qz3kkDKCnXzl+wltIKq5f1SH1HDlv0hoLgikVwg5CLLKCsZ8IFuxur1pdb26uM0vtFp2LJUNad6hK8RsU6p/NeTtOLjbKGsLkqCgSvoPxCAIbFKWIRuAfGd6CrNc2kAD4qM45jAvI9dLuzbopVfhXS16F0i3EzL8/VWuCk7l2mYRdjHAy+9fJksx1zx2wfeEXSoSUX8/ROxpWZaDA8gLAxUrp/hqHU351QDDEdunMfmlrGc6ixyIaxMugRuNsNB4eY91mmbiljeoSCs1GFbVRhC8KejdKpo266hSDdS7f1sV9dnxVhHBhCxWzN7+mfk4KzpjEVFoDR73X8IUOLGFikORl918i86bH2uqJ5zZLvOA4a0BqaRIExmAi7wQrm4iLcDH3THMpvEuy4965JZz1uPJYtGBD/Zj1O2sMA8K6zvSB/8q86fe1VdwIJxOHh50HqAH1jPHHfkxIdrL4nBmvF9Pkzpg/OWlyVjqWmWj0= hh
      "

      # cloudWatch:
      #   clusterLogging:
      #     # enable specific types of cluster control plane logs
      #     enableTypes: ["api", "audit", "authenticator", "controllerManager", "scheduler"]
      #     # all supported types: "api", "audit", "authenticator", "controllerManager", "scheduler"
      #     # supported special values: "*" and "all"
    #+end_src
*** create cluster
    #+begin_src tmate :session hh :window create
      eksctl create cluster --config-file eks-cluster.yaml
   #+end_src

*** get clusters
    #+begin_src shell
      eksctl get clusters
    #+end_src

    #+RESULTS:
    #+begin_example
    NAME		REGION
    apisnoop	ap-southeast-2
    #+end_example

** aws eks list-clusters
   #+begin_src shell
     aws eks list-clusters
   #+end_src


   #+RESULTS:
   #+begin_example
   {
       "clusters": [
           "apisnoop"
       ]
   }
   #+end_example

* ingress
** nginx
https://kubernetes.github.io/ingress-nginx/deploy/#aws
#+begin_src shell
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/aws/service-nlb.yaml
#+end_src

#+RESULTS:
#+begin_example
namespace/ingress-nginx created
configmap/nginx-configuration created
configmap/tcp-services created
configmap/udp-services created
serviceaccount/nginx-ingress-serviceaccount created
clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created
role.rbac.authorization.k8s.io/nginx-ingress-role created
rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created
clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created
deployment.apps/nginx-ingress-controller created
limitrange/ingress-nginx created
service/ingress-nginx created
#+end_example
** nginx-verify
   #+begin_src shell
     POD_NAMESPACE=ingress-nginx
     POD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app.kubernetes.io/name=ingress-nginx -o jsonpath='{.items[0].metadata.name}')

     kubectl exec -it $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version
   #+end_src

   #+RESULTS:
   #+begin_example
   -------------------------------------------------------------------------------
   NGINX Ingress controller
     Release:       0.30.0
     Build:         git-7e65b90c4
     Repository:    https://github.com/kubernetes/ingress-nginx
     nginx version: nginx/1.17.8

   -------------------------------------------------------------------------------

   #+end_example

** ensure load balancer
   #+begin_src shell
     kubectl describe service/ingress-nginx -n ingress-nginx
   #+end_src

   #+RESULTS:
   #+begin_example
   Name:                     ingress-nginx
   Namespace:                ingress-nginx
   Labels:                   app.kubernetes.io/name=ingress-nginx
                             app.kubernetes.io/part-of=ingress-nginx
   Annotations:              kubectl.kubernetes.io/last-applied-configuration:
                               {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"service.beta.kubernetes.io/aws-load-balancer-type":"nlb"},"labels":{"app.k...
                             service.beta.kubernetes.io/aws-load-balancer-type: nlb
   Selector:                 app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx
   Type:                     LoadBalancer
   IP:                       10.100.204.123
   LoadBalancer Ingress:     a1ad497f3010a4d8ca88db5b561d85b2-8c2535b384bbaf36.elb.ap-southeast-2.amazonaws.com
   Port:                     http  80/TCP
   TargetPort:               http/TCP
   NodePort:                 http  32615/TCP
   Endpoints:                192.168.76.191:80
   Port:                     https  443/TCP
   TargetPort:               https/TCP
   NodePort:                 https  31654/TCP
   Endpoints:                192.168.76.191:443
   Session Affinity:         None
   External Traffic Policy:  Local
   HealthCheck NodePort:     30836
   Events:
     Type    Reason                Age   From                Message
     ----    ------                ----  ----                -------
     Normal  EnsuringLoadBalancer  28m   service-controller  Ensuring load balancer
     Normal  EnsuredLoadBalancer   28m   service-controller  Ensured load balancer
   #+end_example

** load balancer dns
You'll want to ensure any hostnames you want to use on this ingress use the following as a CNAME in dns.

   #+name: elb-dnsname
   #+begin_src shell
     kubectl get service/ingress-nginx -n ingress-nginx -o json \
       | jq -r .status.loadBalancer.ingress[0].hostname
   #+end_src

   #+RESULTS: elb-dnsname
   #+begin_example
   a1ad497f3010a4d8ca88db5b561d85b2-8c2535b384bbaf36.elb.ap-southeast-2.amazonaws.com
   #+end_example

** Install Cert-Manager  
   #+begin_src tmate :window cert
     kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.14.1/cert-manager.yaml
   #+end_src

* flux
** flux-manifest.yaml
  #+name: generate k8s manifest for flux
  #+begin_src shell
    export USER=cncf # fork for your own fun
    export NAME="Flux for APISnoop"
    export EMAIL=cncf-ci@ii.coop
    export BRANCH=cncf-ci@ii.coop
            --git-branch=flux \
    export FLUX_FORWARD_NAMESPACE=flux
    fluxctl install \
            --namespace=${FLUX_FORWARD_NAMESPACE} \
            --manifest-generation=true \
            --git-branch=apisnoop.cncf.io \
            --git-email=${EMAIL} \
            --git-label=production \
            --git-path production \
            --git-url=git@github.com:${USER}/apisnoop \
            --git-user="${NAME}" > flux-manifest.yaml
  #+end_src

Some minor changes were made to the yaml:

#+begin_src yaml
        - --git-poll-interval=150
        - --ci-skip
        - --ci-skip-message='[ci skip]'
        - --sync-garbage-collection=true
#+end_src

** deploy
  #+begin_src shell :dir "."
    # possibly editing before you run
    kubectl apply -f flux-manifest.yaml
  #+end_src

  #+RESULTS:
  #+begin_example
  serviceaccount/flux created
  clusterrole.rbac.authorization.k8s.io/flux created
  clusterrolebinding.rbac.authorization.k8s.io/flux created
  deployment.apps/flux created
  secret/flux-git-deploy created
  deployment.apps/memcached created
  service/memcached created
  #+end_example

** delpoyment status
  #+begin_src shell
    kubectl -n ${FLUX_FORWARD_NAMESPACE} rollout status deployment/flux
  #+end_src

  #+RESULTS:
  #+begin_example
  Waiting for deployment "flux" rollout to finish: 0 of 1 updated replicas are available...
  deployment "flux" successfully rolled out
  #+end_example

** ssh-pubkey for deployment
Put this into github as a deploy key for the repo in question and allow write:
https://github.com/cncf/apisnoop/settings/keys/new

  #+begin_src shell
    fluxctl identity || echo FAIL
  #+end_src

  #+RESULTS:
  #+begin_example
  ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzavZXDyMWaNmQWTXXjzmtU5SKBViEk4ym9AFgxoY554E+jE/Kuj7G5iI8gNZccKj76zkn8waG2yojOHqiSbDDpVoupFsr5dosKelxa1ZBQULy9XsJ8PhfCFoI5ovA4B1Pug6bNA/eILD/UVr6WlQXTv0y+8Phh2YQRWl3xr9lvIj5pU+IQcY5aNwvusN1gwh5F++KYSCd+QTVnL3D03GtxIfbiDxR2ow3gW/m6yMg4NtCfG72rVYJrpEYk7FrME+ubM9D0tUcxMY+Q4wIn2snmPjx73KwCXKcnlTw4pptns2ek7KUTBarW7IZ74RVLX62pMhAU+wsRjQ32OO1gxK5rizjduG/8uPcuXj5kp+94sP2Wfa08qT/wljvBm2RzN0+vEa78fU6AIk619UQizqK+C+0uifitSQktBe38iaIwTmbvbJwZzPoyoNow6NRpajJ9to+R02qwvar8HN8VZht1Gk5klEQ3qMQmqlrUpkFd/5tD1d2gp5mGkAiQzXp2yc= root@flux-cc649b69f-m6pdl
  #+end_example

** force sync

You could push and wait for the sync to happen... or fluxctl sync

  #+begin_src tmate :window sync
    fluxctl sync
  #+end_src
* status
** workflows
  #+begin_src shell
    fluxctl list-workloads -n apisnoop
  #+end_src

  #+RESULTS:
  #+begin_example
  WORKLOAD                      CONTAINER           IMAGE                            RELEASE  POLICY
  apisnoop:deployment/hasura    hasura              raiinbow/hasura:v2020.04.08-2    ready    locked
  apisnoop:deployment/pgadmin   pgadmin             dpage/pgadmin4:4.17              ready    locked
                                cp-servers-json     alpine:3.6                                
                                chmod-servers-json  alpine:3.6                                
                                cp-pgpass           alpine:3.6                                
                                chmod-pgpass        alpine:3.6                                
  apisnoop:deployment/postgres  postgres            raiinbow/postgres:v2020.04.08-2  ready    locked
  apisnoop:deployment/webapp    webapp              raiinbow/webapp:v2020.04.08-2    ready    locked
  #+end_example

** images
  #+begin_src shell
    fluxctl list-images -n apisnoop -l 1 2>/dev/null
  #+end_src

  #+RESULTS:
  #+begin_example
  WORKLOAD                      CONTAINER           IMAGE                    CREATED
  apisnoop:deployment/hasura    hasura              raiinbow/hasura          
                                                    |   v2020.04.08-1        07 Apr 20 21:47 UTC
                                                    '-> v2020.04.08-2        07 Apr 20 21:47 UTC
  apisnoop:deployment/pgadmin   pgadmin             dpage/pgadmin4           
                                                    |   snapshot             07 Apr 20 00:01 UTC
                                                    : (11 image(s) omitted)  
                                                    '-> 4.17                 07 Jan 20 13:31 UTC
                                cp-servers-json     alpine                   
                                                    |   3                    23 Mar 20 21:19 UTC
                                                    : (28 image(s) omitted)  
                                                    '-> 3.6                  07 Mar 19 22:20 UTC
                                chmod-servers-json  alpine                   
                                                    |   3                    23 Mar 20 21:19 UTC
                                                    : (28 image(s) omitted)  
                                                    '-> 3.6                  07 Mar 19 22:20 UTC
                                cp-pgpass           alpine                   
                                                    |   3                    23 Mar 20 21:19 UTC
                                                    : (28 image(s) omitted)  
                                                    '-> 3.6                  07 Mar 19 22:20 UTC
                                chmod-pgpass        alpine                   
                                                    |   3                    23 Mar 20 21:19 UTC
                                                    : (28 image(s) omitted)  
                                                    '-> 3.6                  07 Mar 19 22:20 UTC
  apisnoop:deployment/postgres  postgres            raiinbow/postgres        
                                                    |   v2020.04.08-1        07 Apr 20 21:51 UTC
                                                    '-> v2020.04.08-2        07 Apr 20 21:51 UTC
  apisnoop:deployment/webapp    webapp              raiinbow/webapp          
                                                    '-> v2020.04.08-2        07 Apr 20 22:33 UTC
  #+end_example

* cert-manager
** cert
  #+begin_src shell
    kubectl describe certs/letsencrypt-prod -n apisnoop
  #+end_src

** orders
   #+name: order
   #+begin_src shell
     echo -n $(kubectl get orders -A \
       | grep -v NAMESPACE \
       | awk '{print $2}')
   #+end_src

   #+RESULTS: order
   #+begin_example
   letsencrypt-prod-41213390-3382021347
   #+end_example
   #+begin_src shell :result silent
     kubectl describe -n apisnoop order/<<order()>>
   #+end_src

   #+RESULTS:
   #+begin_example
   #+end_example

** challenges
   #+name: challenge
   #+begin_src shell
     echo -n $(kubectl get challenge -A \
       | grep -v NAMESPACE \
       | awk '{print $2}')
   #+end_src

   #+RESULTS: challenge
   #+begin_example
   letsencrypt-prod-41213390-3382021347-1739274342
   #+end_example
   #+begin_src shell :results silent
     kubectl describe -n apisnoop challenge/<<challenge()>>
   #+end_src

* DNS
** dns records
*** Original
**** apisnoop.cncf.io CNAMEs to apisnoop.ii.coop
 #+name: apisnoop.cncf.io dns CNAMEs to IP
 #+begin_src shell
   dig apisnoop.cncf.io | grep -v \; | sed '/^\s*$/d'
 #+end_src

 #+RESULTS: apisnoop.cncf.io dns CNAMEs to IP
 #+begin_src shell
 apisnoop.cncf.io.	115	IN	CNAME	apisnoop.ii.coop.
 apisnoop.ii.coop.	25	IN	CNAME	apisnoop.cncf.ci.
 apisnoop.cncf.ci.	25	IN	CNAME	www.apisnoop.io.
 www.apisnoop.io.	116	IN	A	35.189.36.228
 #+end_src
**** apisnoop.ii.coop CNAMEs to apisnoop.cncf.ci
 #+name: apisnoop.ii.coop dns CNAMEs to IP
 #+begin_src shell
   dig apisnoop.ii.coop | grep -v \; | sed '/^\s*$/d'
 #+end_src

 #+RESULTS: apisnoop.ii.coop dns CNAMEs to IP
 #+begin_src shell
 apisnoop.ii.coop.	59	IN	CNAME	apisnoop.cncf.ci.
 apisnoop.cncf.ci.	59	IN	CNAME	www.apisnoop.io.
 www.apisnoop.io.	299	IN	A	35.189.36.228
 #+end_src

**** apisnoop.cncf.ci CNAMEs to www.apisnoop.io
 #+name: apisnoop.cncf.ci dns CNAMEs to IP
 #+begin_src shell
   dig apisnoop.cncf.ci | grep -v \; | sed '/^\s*$/d'
 #+end_src

 #+RESULTS: apisnoop.cncf.ci dns CNAMEs to IP
 #+begin_src shell
 apisnoop.cncf.ci.	59	IN	CNAME	www.apisnoop.io.
 www.apisnoop.io.	299	IN	A	35.189.36.228
 #+end_src

**** www.apisnoop.io A to 35.189.36.228
 #+name: www.apisnoop.io dns CNAMEs to IP
 #+begin_src shell
   dig www.apisnoop.io | grep -v \; | sed '/^\s*$/d'
 #+end_src

 #+RESULTS: www.apisnoop.io dns CNAMEs to IP
 #+begin_src shell
 www.apisnoop.io.	299	IN	A	35.189.36.228
 #+end_src

 #+RESULTS: apisnoop.cncf.ci dns CNAMEs to IP
 #+begin_src shell
 apisnoop.cncf.ci.	59	IN	CNAME	www.apisnoop.io.
 www.apisnoop.io.	299	IN	A	35.189.36.228
 #+end_src

** dns domains IPs
*** dns servers for cncf.io
#+name: cncf.io registrar
#+begin_src shell
  whois cncf.io | grep Name\ Server:
#+end_src

#+RESULTS: cncf.io registrar
#+begin_src shell
Name Server: NS2.DNSIMPLE.COM
Name Server: NS1.DNSIMPLE.COM
Name Server: NS3.DNSIMPLE.COM
Name Server: NS4.DNSIMPLE.COM
#+end_src

*** dns servers for ii.coop
#+name: ii.coop registrar
#+begin_src shell
  whois ii.coop | grep Name\ Server:
#+end_src

#+RESULTS: ii.coop registrar
#+begin_src shell
Name Server: NS4.DNSIMPLE.COM
Name Server: NS3.DNSIMPLE.COM
Name Server: NS2.DNSIMPLE.COM
Name Server: NS1.DNSIMPLE.COM
Name Server: ns4.dnsimple.com
Name Server: ns3.dnsimple.com
Name Server: ns2.dnsimple.com
Name Server: ns1.dnsimple.com
#+end_src

*** dns servers for cncf.ci
#+name: cncf.ci registrar
#+begin_src shell
  whois cncf.ci | grep Name\ Server:
#+end_src

#+RESULTS: cncf.ci registrar
#+begin_src shell
Name Server: ns1.dnsimple.com
Name Server: ns2.dnsimple.com
Name Server: ns3.dnsimple.com
Name Server: ns4.dnsimple.com
#+end_src

*** dns servers for apisnoop.io
#+name: apisnoop.io registrar
#+begin_src shell
  whois apisnoop.io | grep Name\ Server:
#+end_src

#+RESULTS: apisnoop.io registrar
#+begin_example
Name Server: NS-1646.AWSDNS-13.CO.UK
Name Server: NS-424.AWSDNS-53.COM
Name Server: NS-878.AWSDNS-45.NET
Name Server: NS-1407.AWSDNS-47.ORG
#+end_example
** Debugging
   #+name: services currently hosted by google
   #+begin_src shell
      whois 35.189.36.228 | grep -i organization
   #+end_src

   #+RESULTS: services currently hosted by google
   #+begin_src shell
   Organization:   Google LLC (GOOGL-2)
   #+end_src
* Footnotes
** alb-ingress
I'm not sure we need this
  #+begin_src tmate :window ingress
     # brew install helm
     helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator
  #+end_src
  #+begin_src tmate :window ingress
     helm install alb-ingress \
      incubator/aws-alb-ingress-controller \
      --set autoDiscoverAwsRegion=true \
      --set autoDiscoverAwsVpcID=true \
      --set clusterName=apisnoop
  #+end_src
** image building
   #+begin_src tmate :window image
     for APP in auditlogger webapp postgres hasura ; do
         cd ~/apisnoop/apps/$APP/app
         export IMAGE=raiinbow/$APP:v2020.04.08-3
         docker build -t $IMAGE .
         docker push $IMAGE
     done
   #+end_src
