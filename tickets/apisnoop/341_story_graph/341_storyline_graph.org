#+TITLE: Ticket 341: A storyline graph

* Ticket
  #+begin_quote
Create an initial visual story of conformance test writing--a straightforward infographic that shows how test writing has increased over time, and how these increases map to important dates in conformance testing and apisnoop.

Important Dates include:

2018 TOC meeting in LA, where oomici pointed out the scary tests / endpoints ratio .... 9-11%
ii hired for APISnoop tooling
Globant hired to write tests
ii hired to write tests
process started of org-file mock tickets that convert to real tickets.

    k8s releases

For each point, The graph would display:

total conformance eligible endpoints

    total endpoints covered by conformance tests.

With this, one should be able to easily comprehend how test-writing has increased over time, and any velocity increase.

The timeline would start at k8s v.1.9 to today, v.1.19 and show However, it was not possible to easily determine testing coverage until changes put in place in 1.12...and so coverage would show as 0 until v.1.12
  #+end_quote
* Strategy
  - Gather older data for 1.9 through 1.14
  - calculate total eligible endpoints for each set
  - calculate conformance hits for each set
  - writed this data to a json file to be used in new webpage
  - create a route in webapp called '/timeline'
  - in timeline make graph, based loosely on our coverage over time graph, that uses our json as its data
  - incorporate interactivity for the timeline events
* Process
** Gather older data
   We still have buckets of processed data in our google cloud bucket.
   This bucket, specifically, seems like a good resource: [[https://console.cloud.google.com/storage/browser/apisnoop/dev/?forceOnBucketsSortingFiltering=false&project=apisnoop][gs://apisnoop/dev]]
   as it includes processed data for 1.9 through 1.12.
   The processed data is from our earlier time where it is all done as .json, like ~endpoints.json~ and ~tests.json~ etc.  ~endpoints.json~ shows each endpoint and its number of conf hits.  We can parse this data to get total eligible endopints and the conformance coverage.

   I would expect conformance hits to be 0 until we get to 1.12.  If that's not the case, i will need to check with hh about how these numbers were generated.

   For processing, I will be using clojure, as its the sweetest environment for working in.  To adi in this, i'll copy the endpoint.json down to a data folder in this directory

  #+NAME: Copy down endpoints.json for each release
  #+begin_src shell
    mkdir data
    #1.9
    gsutil cp \
           gs://apisnoop/dev/1.9/ci-kubernetes-e2e-gce-cos-k8sstable3-default-357/endpoints.json \
           data/1.9_endpoints.json
    #1.10
    gsutil cp \
           gs://apisnoop/dev/1.10/ci-kubernetes-e2e-gce-cos-k8sstable2-default-1398/endpoints.json \
           data/1.10_endpoints.json
    #1.11
    gsutil cp \
           gs://apisnoop/dev/1.11/ci-kubernetes-e2e-gce-cos-k8sstable1-default-4123/endpoints.json \
           data/1.11_endpoints.json
    #1.12
    gsutil cp \
           gs://apisnoop/dev/1.12/ci-kubernetes-e2e-gce-cos-k8sbeta-default-6525/endpoints.json \
           data/1.12_endpoints.json
    #1.13
    gsutil cp \
           gs://apisnoop/dev/ci-kubernetes-e2e-gce-cos-k8sstable3-default/1163145237766344704/endpoints.json \
           data/1.13_endpoints.json
    #1.14
    gsutil cp \
           gs://apisnoop/dev/ci-kubernetes-e2e-gce-cos-k8sstable2-default/1163516685768986625/endpoints.json \
           data/1.14_endpoints.json
    tree
  #+end_src

  #+RESULTS: Copy down endpoints.json for each release
  #+begin_example
  .
  ├── 341_storyline_graph.org
  ├── data
  │   ├── 1.10_endpoints.json
  │   ├── 1.11_endpoints.json
  │   ├── 1.12_endpoints.json
  │   ├── 1.13_endpoints.json
  │   ├── 1.14_endpoints.json
  │   └── 1.9_endpoints.json
  └── parse_data.clj

  1 directory, 8 files
  #+end_example
** Explore data using babashka
   [[https://github.com/borkdude/babashka][babashka]] lets us write clojure as shell scripts by adding ~#!/usr/bin/env bb~ to the top
   We wrote a script at [[file:parse_data.clj][parse_data.clj]] to generate json of our key stats
   With this we can run a script like so:

  #+NAME: generate stats.json
   #+begin_src shell
   ./parse_data.clj
   #+end_src

   #+RESULTS: generate stats.json
   #+begin_example
     [ {
       "confTested" : {
         "eligible" : 0,
         "eligibleStable" : 0,
         "stable" : 0,
         "total" : 0
       },
       "date" : "2017-12-15",
       "release" : "1.9",
       "endpoints" : {
         "eligible" : 942,
         "eligibleStable" : 465,
         "stable" : 481,
         "total" : 958
       },
       "tested" : {
         "eligible" : 0,
         "eligibleStable" : 0,
         "stable" : 0,
         "total" : 0
       }
     }, {
       "confTested" : {
         "eligible" : 0,
         "eligibleStable" : 0,
         "stable" : 0,
         "total" : 0
       },
       "date" : "2018-03-28",
       "release" : "1.10",
       "endpoints" : {
         "eligible" : 929,
         "eligibleStable" : 434,
         "stable" : 450,
         "total" : 945
       },
       "tested" : {
         "eligible" : 0,
         "eligibleStable" : 0,
         "stable" : 0,
         "total" : 0
       }
     }, {
       "confTested" : {
         "eligible" : 0,
         "eligibleStable" : 0,
         "stable" : 0,
         "total" : 0
       },
       "date" : "2018-07-03",
       "release" : "1.11",
       "endpoints" : {
         "eligible" : 947,
         "eligibleStable" : 436,
         "stable" : 452,
         "total" : 963
       },
       "tested" : {
         "eligible" : 0,
         "eligibleStable" : 0,
         "stable" : 0,
         "total" : 0
       }
     }, {
       "confTested" : {
         "eligible" : 115,
         "eligibleStable" : 77,
         "stable" : 77,
         "total" : 115
       },
       "date" : "2018-09-27",
       "release" : "1.12",
       "endpoints" : {
         "eligible" : 786,
         "eligibleStable" : 357,
         "stable" : 373,
         "total" : 802
       },
       "tested" : {
         "eligible" : 193,
         "eligibleStable" : 121,
         "stable" : 122,
         "total" : 194
       }
     }, {
       "confTested" : {
         "eligible" : 111,
         "eligibleStable" : 76,
         "stable" : 76,
         "total" : 111
       },
       "date" : "2018-12-03",
       "release" : "1.13",
       "endpoints" : {
         "eligible" : 795,
         "eligibleStable" : 358,
         "stable" : 384,
         "total" : 821
       },
       "tested" : {
         "eligible" : 197,
         "eligibleStable" : 131,
         "stable" : 131,
         "total" : 197
       }
     }, {
       "confTested" : {
         "eligible" : 125,
         "eligibleStable" : 82,
         "stable" : 82,
         "total" : 125
       },
       "date" : "2019-03-25",
       "release" : "1.14",
       "endpoints" : {
         "eligible" : 847,
         "eligibleStable" : 376,
         "stable" : 402,
         "total" : 873
       },
       "tested" : {
         "eligible" : 212,
         "eligibleStable" : 142,
         "stable" : 142,
         "total" : 212
       }
     } ]
   #+end_example

   All the data is gathered from our google cloud bucket, and so we are trustiong that the numbers generated back then are accurate.  This wnds up being a 'close-enough' graph.

   Now, we wanna output this json to a file in our webapp that we can use for our graph.  We will need to add 1.15-1.19, which we can do directly from our sql queries.

   As I need to generate new data anyway, I am going to run this query on our main server and then paste the results back here.
** Results for 1.15 through 1.19
   I created a json from the main server  (see [[file:~/apisnoop/org/explorations/postgres_to_json.org][explorations: "Postgres to JSON"]] )
   Resulting in this:
  #+NAME: Results for 1.15 through 1.19
  #+begin_example json
[{
    "release": "1.15.13",
    "date": "2019-06-20",
    "endpoints": {
        "total": 873,
        "stable": 402,
        "eligible": 824,
        "eligibleStable": 367
    },
    "tested": {
        "total": 214,
        "stable": 140,
        "eligible": 204,
        "eligibleStable": 131
    },
    "confTested": {
        "total": 126,
        "stable": 86,
        "eligible": 124,
        "eligibleStable": 84
    }
}, {
    "release": "1.16.10",
    "date": "2019-10-22",
    "endpoints": {
        "total": 910,
        "stable": 430,
        "eligible": 861,
        "eligibleStable": 395
    },
    "tested": {
        "total": 222,
        "stable": 172,
        "eligible": 212,
        "eligibleStable": 163
    },
    "confTested": {
        "total": 146,
        "stable": 118,
        "eligible": 144,
        "eligibleStable": 116
    }
}, {
    "release": "1.17.6",
    "date": "2019-12-09",
    "endpoints": {
        "total": 939,
        "stable": 438,
        "eligible": 890,
        "eligibleStable": 403
    },
    "tested": {
        "total": 217,
        "stable": 173,
        "eligible": 207,
        "eligibleStable": 163
    },
    "confTested": {
        "total": 156,
        "stable": 131,
        "eligible": 154,
        "eligibleStable": 129
    }
}, {
    "release": "1.18.1",
    "date": "2020-03-24",
    "endpoints": {
        "total": 795,
        "stable": 445,
        "eligible": 746,
        "eligibleStable": 410
    },
    "tested": {
        "total": 229,
        "stable": 181,
        "eligible": 219,
        "eligibleStable": 171
    },
    "confTested": {
        "total": 165,
        "stable": 140,
        "eligible": 163,
        "eligibleStable": 138
    }
}, {
    "release": "1.19.0",
    "date": "2020-05-19",
    "endpoints": {
        "total": 795,
        "stable": 445,
        "eligible": 746,
        "eligibleStable": 410
    },
    "tested": {
        "total": 238,
        "stable": 190,
        "eligible": 228,
        "eligibleStable": 180
    },
    "confTested": {
        "total": 168,
        "stable": 144,
        "eligible": 166,
        "eligibleStable": 142
    }
}]

  #+end_example

 I then combined the two json arrays into a singble one called "release_stats.json", which I'll copy into our webapp  (you can also view it in this dir).

* Conclusion
