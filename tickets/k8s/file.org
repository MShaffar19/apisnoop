# -*- ii: apisnoop; -*-
#+TITLE: Mock Ticket Template
#+AUTHOR: ii team
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(b) | DONE(d)
#+OPTIONS: toc:nil tags:nil todo:nil
#+EXPORT_SELECT_TAGS: export
* TODO Progress [2/5]                                                :export:
- [X] APISnoop org-flow : [[https://github.com/cncf/apisnoop/blob/master/tickets/k8s/][MyEndpoint.org]]
- [X] test approval issue : [[https://github.com/kubernetes/kubernetes/issues/][kubernetes/kubernetes#]]
- [ ] test pr : kuberenetes/kubernetes#
- [ ] two weeks soak start date : testgrid-link
- [ ] two weeks soak end date :
- [ ] test promotion pr : kubernetes/kubernetes#?
* Identifying an untested feature Using APISnoop                     :export:

According to this APIsnoop query, there are still some remaining RESOURCENAME endpoints which are untested.

#+NAME: untested_stable_core_endpoints
  #+begin_src sql-mode :eval never-export :exports both :session none
  SELECT
    -- operation_id,
    -- k8s_action,
    -- path,
    -- description,
    -- kind
    distinct kind, operation_id
    FROM untested_stable_core_endpoints
    -- FROM untested_stable_endpoints
    where path not like '%volume%'
    and kind not like '%Options'
    and kind not like '%ComponentStatus%'
    -- and kind like ''
    -- and operation_id ilike '%%'
    and operation_id not ilike '%NodeProxy%'
   ORDER BY kind,operation_id desc
   LIMIT 25
         ;
  #+end_src

 #+RESULTS: untested_stable_core_endpoints
 #+begin_SRC example
       kind       |                operation_id                
 -----------------+--------------------------------------------
  Binding         | createCoreV1NamespacedPodBinding
  Binding         | createCoreV1NamespacedBinding
  ComponentStatus | readCoreV1ComponentStatus
  ComponentStatus | listCoreV1ComponentStatus
  Event           | replaceCoreV1NamespacedEvent
  Event           | readCoreV1NamespacedEvent
  Event           | patchCoreV1NamespacedEvent
  Event           | listCoreV1NamespacedEvent
  Event           | listCoreV1EventForAllNamespaces
  Event           | deleteCoreV1NamespacedEvent
  Event           | deleteCoreV1CollectionNamespacedEvent
  Event           | createCoreV1NamespacedEvent
  LimitRange      | patchCoreV1NamespacedLimitRange
  LimitRange      | listCoreV1LimitRangeForAllNamespaces
  LimitRange      | deleteCoreV1CollectionNamespacedLimitRange
  Namespace       | replaceCoreV1NamespaceStatus
  Namespace       | replaceCoreV1NamespaceFinalize
  Namespace       | readCoreV1NamespaceStatus
  Namespace       | patchCoreV1NamespaceStatus
  Namespace       | patchCoreV1Namespace
  Namespace       | listCoreV1Namespace
  Node            | replaceCoreV1NodeStatus
  Node            | readCoreV1NodeStatus
  Node            | patchCoreV1NodeStatus
  Node            | deleteCoreV1Node
 (25 rows)

 #+end_SRC

* API Reference and feature documentation                            :export:
- [[https://kubernetes.io/docs/reference/kubernetes-api/][Kubernetes API Reference Docs]]
- [[https://github.com/kubernetes/client-go/blob/master/kubernetes/typed/core/v1/RESOURCENAME.go][client-go - RESOURCENAME]] 

* The mock test                                                      :export:
** Test outline
1. Create a RESOURCENAME with a static label

2. Patch the RESOURCENAME with a new label and updated data

3. Get the RESOURCENAME to ensure it's patched

4. List all RESOURCENAMEs in all Namespaces with a static label
   find the RESOURCENAME
   ensure that the RESOURCENAME is found and is patched

5. Delete Namespaced RESOURCENAME via a Collection with a LabelSelector

** Test the functionality in Go
   #+begin_src go :wrap "example"
    package main

import (
  "encoding/json"
  "fmt"
  "flag"
  "os"
  v1 "k8s.io/api/core/v1"
  appsv1 "k8s.io/api/apps/v1"
  // "k8s.io/client-go/dynamic"
  // "k8s.io/apimachinery/pkg/runtime/schema"
  metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
  "k8s.io/client-go/kubernetes"
  "k8s.io/apimachinery/pkg/types"
  "k8s.io/client-go/tools/clientcmd"
  watch "k8s.io/apimachinery/pkg/watch"
)

func main() {
  // uses the current context in kubeconfig
  kubeconfig := flag.String("kubeconfig", fmt.Sprintf("%v/%v/%v", os.Getenv("HOME"), ".kube", "config"), "(optional) absolute path to the kubeconfig file")
  flag.Parse()
  config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
  if err != nil {
      fmt.Println(err)
      return
  }
  // make our work easier to find in the audit_event queries
  config.UserAgent = "live-test-writing"
  // creates the clientset
  ClientSet, _ := kubernetes.NewForConfig(config)
  // DynamicClientSet, _ := dynamic.NewForConfig(config)
  // podResource := schema.GroupVersionResource{Group: "", Version: "v1", Resource: "pods"}

  // TEST BEGINS HERE

  testDaemonSetName := "testdaemonset"
  testDaemonSetImageInitial := "nginx"
  testDaemonSetImagePatch := "alpine"
  testDaemonSetImageUpdate := "httpd"
  testDaemonSetStaticLabel := map[string]string{"test-static": "true"}
  testDaemonSetStaticLabelFlat := "test-static=true"
  testDaemonSetSelector := map[string]string{"app": testDaemonSetName}
  testNamespaceName := "default"

  fmt.Println("creating a DaemonSet")
  testDaemonSet := appsv1.DaemonSet{
      ObjectMeta: metav1.ObjectMeta{
          Name: testDaemonSetName,
          Labels: testDaemonSetStaticLabel,
      },
      Spec: appsv1.DaemonSetSpec{
          Selector: &metav1.LabelSelector{
              MatchLabels: testDaemonSetSelector,
          },
          Template: v1.PodTemplateSpec{
              ObjectMeta: metav1.ObjectMeta{
                  Labels: testDaemonSetSelector,
              },
              Spec: v1.PodSpec{
                  Containers: []v1.Container{{
                      Name: testDaemonSetName,
                      Image: testDaemonSetImageInitial,
                  }},
              },
          },
      },
  }
  _, err = ClientSet.AppsV1().DaemonSets(testNamespaceName).Create(&testDaemonSet)

  fmt.Println("watching for the DaemonSet to be added")
  resourceWatchTimeoutSeconds := int64(180)
  resourceWatch, err := ClientSet.AppsV1().DaemonSets(testNamespaceName).Watch(metav1.ListOptions{LabelSelector: testDaemonSetStaticLabelFlat, TimeoutSeconds: &resourceWatchTimeoutSeconds})
  if err != nil {
      fmt.Println(err, "failed to setup watch on newly created DaemonSet")
      return
  }

  resourceWatchChan := resourceWatch.ResultChan()
  for watchEvent := range resourceWatchChan {
      if watchEvent.Type == watch.Added {
          break
      }
  }	
  fmt.Println("watching for DaemonSet readiness count to be equal to the desired count")
  for watchEvent := range resourceWatchChan {
      daemonset, ok := watchEvent.Object.(*appsv1.DaemonSet)
      if ok == false {
          fmt.Println("failed to convert watchEvent.Object type")
          return
      }
      if daemonset.Status.NumberReady == daemonset.Status.DesiredNumberScheduled {
          break
      }
  }	
  defer func() {
      fmt.Println("deleting the DaemonSet")
      err = ClientSet.AppsV1().DaemonSets(testNamespaceName).DeleteCollection(&metav1.DeleteOptions{}, metav1.ListOptions{LabelSelector: testDaemonSetStaticLabelFlat})
  _, err = ClientSet.AppsV1().DaemonSets(testNamespaceName).Create(&testDaemonSet)
      if err != nil {
          fmt.Println(err)
          return
      }
      for watchEvent := range resourceWatchChan {
          daemonset, ok := watchEvent.Object.(*appsv1.DaemonSet)
          if ok != true {
              fmt.Println("unable to convert watchEvent.Object type")
              return
          }
          if watchEvent.Type == watch.Deleted && daemonset.ObjectMeta.Name == testDaemonSetName {
              break
          }
      }
  }()

  fmt.Println("patching the DaemonSet")
  resourcePatch, err := json.Marshal(map[string]interface{}{
      "metadata": map[string]interface{}{
          "labels": map[string]string{"test-resource": "patched"},
      },
      "spec": map[string]interface{}{
          "template": map[string]interface{}{
              "spec": map[string]interface{}{
                  "containers": []map[string]interface{}{{
                      "name": testDaemonSetName,
                      "image": testDaemonSetImagePatch,
                      "command": []string{"/bin/sleep", "100000"},
                  }},
              },
          },
      },
  })
  if err != nil {
      fmt.Println(err, "failed marshal resource patch")
      return
  }
  _, err = ClientSet.AppsV1().DaemonSets(testNamespaceName).Patch(testDaemonSetName, types.StrategicMergePatchType, []byte(resourcePatch))
  if err != nil {
      fmt.Println(err, "failed patch resource")
      return
  }
  for watchEvent := range resourceWatchChan {
     if watchEvent.Type == watch.Modified {
         break
     }
  }
  fmt.Println("watching for DaemonSet readiness count to be equal to the desired count")
  for watchEvent := range resourceWatchChan {
      daemonset, ok := watchEvent.Object.(*appsv1.DaemonSet)
      if ok == false {
          fmt.Println("failed to convert watchEvent.Object type")
          return
      }
      if daemonset.Status.NumberReady == daemonset.Status.DesiredNumberScheduled {
          break
      }
  }	

  fmt.Println("fetching the DaemonSet")
  ds, err := ClientSet.AppsV1().DaemonSets(testNamespaceName).Get(testDaemonSetName, metav1.GetOptions{})
  if err != nil {
      fmt.Println(err, "failed fetch resource")
      return
  }
  if ds.ObjectMeta.Labels["test-resource"] != "patched" {
      fmt.Println("failed to patch resource - missing patched label")
      return
  }
  if ds.Spec.Template.Spec.Containers[0].Image != testDaemonSetImagePatch {
      fmt.Println("failed to patch resource - missing patched image")
      return
  }
  if ds.Spec.Template.Spec.Containers[0].Command[0] != "/bin/sleep" {
      fmt.Println("failed to patch resource - missing patched command")
      return
  }

  fmt.Println("updating the DaemonSet")
  dsUpdate := ds
  dsUpdate.ObjectMeta.Labels["test-resource"] = "updated"
  dsUpdate.Spec.Template.Spec.Containers[0].Image = testDaemonSetImageUpdate
  dsUpdate.Spec.Template.Spec.Containers[0].Command = []string{}
  _, err = ClientSet.AppsV1().DaemonSets(testNamespaceName).Update(dsUpdate)
  if err != nil {
      fmt.Println(err, "failed to update resource")
      return
  }
  fmt.Println("watching for DaemonSet readiness count to be equal to the desired count")
  for watchEvent := range resourceWatchChan {
      daemonset, ok := watchEvent.Object.(*appsv1.DaemonSet)
      if ok == false {
          fmt.Println("failed to convert watchEvent.Object type")
          return
      }
      if daemonset.Status.NumberReady == daemonset.Status.DesiredNumberScheduled {
          break
      }
  }	

  fmt.Println("listing DaemonSets")
  dss, err := ClientSet.AppsV1().DaemonSets("").List(metav1.ListOptions{LabelSelector: testDaemonSetStaticLabelFlat})
  if err != nil {
      fmt.Println(err, "failed to list DaemonSets")
      return
  }
  if len(dss.Items) == 0 {
      fmt.Println("there are no DaemonSets found")
      return
  }
  for _, ds := range dss.Items {
      if ds.ObjectMeta.Labels["test-resource"] != "updated" {
          fmt.Println("failed to patch resource - missing updated label")
          return
      }
      if ds.Spec.Template.Spec.Containers[0].Image != testDaemonSetImageUpdate {
          fmt.Println("failed to patch resource - missing updated image")
          return
      }
      if len(ds.Spec.Template.Spec.Containers[0].Command) != 0 {
          fmt.Println("failed to patch resource - missing updated command")
          return
      }
  }

  // TEST ENDS HERE

  // write test here
  fmt.Println("[status] complete")

}
   #+end_src

   #+RESULTS:
   #+begin_example
   creating a DaemonSet
   watching for the DaemonSet to be added
   watching for DaemonSet readiness count to be equal to the desired count
   patching the DaemonSet
   watching for DaemonSet readiness count to be equal to the desired count
   fetching the DaemonSet
   updating the DaemonSet
   watching for DaemonSet readiness count to be equal to the desired count
   listing DaemonSets
   [status] complete
   deleting the DaemonSet
   #+end_example

* Verifying increase in coverage with APISnoop                       :export:
Discover useragents:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select distinct useragent from audit_event where bucket='apisnoop' and useragent not like 'kube%' and useragent not like 'coredns%' and useragent not like 'kindnetd%' and useragent like 'live%';
  #+end_src

List endpoints hit by the test:
#+begin_src sql-mode :exports both :session none
select * from endpoints_hit_by_new_test where useragent like 'live%'; 
#+end_src

#+RESULTS:
#+begin_SRC example
     useragent     |               operation_id                | hit_by_ete | hit_by_new_test 
-------------------+-------------------------------------------+------------+-----------------
 live-test-writing | createAppsV1NamespacedDaemonSet           | f          |               1
 live-test-writing | createCoreV1NamespacedPod                 | t          |               1
 live-test-writing | deleteAppsV1CollectionNamespacedDaemonSet | f          |               1
 live-test-writing | deleteCoreV1NamespacedPod                 | t          |               1
 live-test-writing | listAppsV1DaemonSetForAllNamespaces       | f          |               1
 live-test-writing | listAppsV1NamespacedDaemonSet             | t          |               1
 live-test-writing | listCoreV1PodForAllNamespaces             | f          |               1
 live-test-writing | patchAppsV1NamespacedDaemonSet            | f          |               1
 live-test-writing | readAppsV1NamespacedDaemonSet             | f          |               1
 live-test-writing | replaceAppsV1NamespacedDaemonSet          | f          |               1
(10 rows)

#+end_SRC

Display endpoint coverage change:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select * from projected_change_in_coverage;
  #+end_src

  #+RESULTS:
  #+begin_SRC example
     category    | total_endpoints | old_coverage | new_coverage | change_in_number 
  ---------------+-----------------+--------------+--------------+------------------
   test_coverage |             445 |          188 |          195 |                7
  (1 row)

  #+end_SRC

* Final notes :export:
If a test with these calls gets merged, **test coverage will go up by N points**

This test is also created with the goal of conformance promotion.

-----  
/sig testing  

/sig architecture  

/area conformance  

