# -*- ii: apisnoop; -*-
#+TITLE: Mock Ticket Template
#+AUTHOR: ii team
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(b) | DONE(d)
#+OPTIONS: toc:nil tags:nil todo:nil
#+EXPORT_SELECT_TAGS: export
* TODO Progress [2/5]                                                :export:
- [X] APISnoop org-flow : [[https://github.com/cncf/apisnoop/blob/proxyNodeTest/tickets/k8s/proxyNode_v1.org][nodeProxy.org]]
- [ ] test approval issue : [[https://github.com/kubernetes/kubernetes/issues/][kubernetes/kubernetes#]]
- [ ] test pr : kuberenetes/kubernetes#
- [ ] two weeks soak start date : testgrid-link
- [ ] two weeks soak end date :
- [ ] test promotion pr : kubernetes/kubernetes#?
* Identifying an untested feature Using APISnoop                     :export:

According to this APIsnoop query, there are still some remaining RESOURCENAME endpoints which are untested.

  #+NAME: untested_stable_core_endpoints
  #+begin_src sql-mode :eval never-export :exports both :session none
    SELECT
      operation_id,
      -- k8s_action,
      -- path,
      -- description,
      kind
      FROM untested_stable_core_endpoints
      -- FROM untested_stable_endpoints
      where path not like '%volume%'
      -- and kind like 'NodeProxy%'
      and kind like 'NodeProxyOptions'
      -- and operation_id ilike '%nodeProxy%'
     ORDER BY kind,operation_id desc
     LIMIT 25
           ;
  #+end_src

 #+RESULTS: untested_stable_core_endpoints
 #+begin_SRC example
              operation_id              |       kind       
 ---------------------------------------+------------------
  connectCoreV1PutNodeProxyWithPath     | NodeProxyOptions
  connectCoreV1PutNodeProxy             | NodeProxyOptions
  connectCoreV1PostNodeProxyWithPath    | NodeProxyOptions
  connectCoreV1PostNodeProxy            | NodeProxyOptions
  connectCoreV1PatchNodeProxyWithPath   | NodeProxyOptions
  connectCoreV1PatchNodeProxy           | NodeProxyOptions
  connectCoreV1OptionsNodeProxyWithPath | NodeProxyOptions
  connectCoreV1OptionsNodeProxy         | NodeProxyOptions
  connectCoreV1HeadNodeProxyWithPath    | NodeProxyOptions
  connectCoreV1HeadNodeProxy            | NodeProxyOptions
  connectCoreV1GetNodeProxy             | NodeProxyOptions
  connectCoreV1DeleteNodeProxyWithPath  | NodeProxyOptions
  connectCoreV1DeleteNodeProxy          | NodeProxyOptions
 (13 rows)

 #+end_SRC

* API Reference and feature documentation                            :export:
** kube-proxy k8s node binary/service

https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/ T
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#-strong-proxy-operations-node-v1-core-strong-
- [[https://kubernetes.io/docs/reference/kubernetes-api/][Kubernetes API Reference Docs]]
- [[https://github.com/kubernetes/client-go/blob/master/kubernetes/typed/core/v1/RESOURCENAME.go][client-go - RESOURCENAME]] 
https://apisnoop.cncf.io/ci-kubernetes-e2e-gci-gce/1252317920617304064/stable/core/connectCoreV1PatchNodeProxy
* The mock test                                                      :export:
** Test outline
1. Create a RESOURCENAME with a static label

2. Patch the RESOURCENAME with a new label and updated data

3. Get the RESOURCENAME to ensure it's patched

4. List all RESOURCENAMEs in all Namespaces with a static label
   find the RESOURCENAME
   ensure that the RESOURCENAME is found and is patched

5. Delete Namespaced RESOURCENAME via a Collection with a LabelSelector

** Test the functionality in Go
   #+begin_src go
     package main

     import (
       // "encoding/json"
       "fmt"
       "flag"
       "os"
       v1 "k8s.io/api/core/v1"
       // "k8s.io/client-go/dynamic"
       // "k8s.io/apimachinery/pkg/runtime/schema"
       metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
       "k8s.io/client-go/kubernetes"
       // "k8s.io/apimachinery/pkg/types"
       "k8s.io/client-go/tools/clientcmd"
     )

     func main() {
       // uses the current context in kubeconfig
       kubeconfig := flag.String("kubeconfig", fmt.Sprintf("%v/%v/%v", os.Getenv("HOME"), ".kube", "config"), "(optional) absolute path to the kubeconfig file")
       flag.Parse()
       config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
       if err != nil {
           fmt.Println(err)
           return
       }
       // make our work easier to find in the audit_event queries
       config.UserAgent = "live-test-writing"
       // creates the clientset
       ClientSet, _ := kubernetes.NewForConfig(config)
       // DynamicClientSet, _ := dynamic.NewForConfig(config)
       // podResource := schema.GroupVersionResource{Group: "", Version: "v1", Resource: "pods"}

       // TEST BEGINS HERE

       testPodName := "test-pod"
       testPodImage := "nginx"
       testNamespaceName := "default"

       fmt.Println("creating a Pod")
       testPod := v1.Pod{
         ObjectMeta: metav1.ObjectMeta{
           Name: testPodName,
           Labels: map[string]string{"test-pod-static": "true"},
         },
         Spec: v1.PodSpec{
           Containers: []v1.Container{{
             Name: testPodName,
             Image: testPodImage,
           }},
         },
       }
       _, err = ClientSet.CoreV1().Pods(testNamespaceName).Create(&testPod)
       if err != nil {
           fmt.Println(err, "failed to create Pod")
           return
       }

       fmt.Println("listing Pods")
       pods, err := ClientSet.CoreV1().Pods("").List(metav1.ListOptions{LabelSelector: "test-pod-static=true"})
       if err != nil {
           fmt.Println(err, "failed to list Pods")
           return
       }
       podCount := len(pods.Items)
       if podCount == 0 {
           fmt.Println("there are no Pods found")
           return
       }
       fmt.Println(podCount, "Pod(s) found")

       fmt.Println("deleting Pod")
       err = ClientSet.CoreV1().Pods(testNamespaceName).Delete(testPodName, &metav1.DeleteOptions{})
       if err != nil {
           fmt.Println(err, "failed to delete the Pod")
           return
       }

       // TEST ENDS HERE

       fmt.Println("[status] complete")

     }
   #+end_src

   #+RESULTS:
   #+begin_src go
   creating a Pod
   listing Pods
   1 Pods found
   deleting Pod
   [status] complete
   #+end_src

* See what other apps hit NodeProxyOptions
** Discover useragents live in cluster
  #+begin_src sql-mode :eval never-export :exports both :session none
    select distinct useragent from audit_event where bucket='apisnoop' 
     and useragent not like 'kube%'
     and useragent not like 'coredns%'
     and useragent not like 'kindnetd%'
     -- and useragent like 'live%'
     ;
  #+end_src

  #+RESULTS:
  #+begin_SRC example
                              useragent                             
  ------------------------------------------------------------------
   local-path-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format
   nginx-ingress-controller/v0.0.0 (linux/amd64) kubernetes/$Format
   tilt/v0.0.0 (linux/amd64) kubernetes/$Format
  (3 rows)

  #+end_SRC
** describe the audit_event

  #+begin_src sql-mode :eval never-export :exports both :session none
    \d audit_event
  #+end_src

  #+RESULTS:
  #+begin_SRC example
                    Unlogged table "public.audit_event"
             Column           |  Type   | Collation | Nullable | Default 
  ----------------------------+---------+-----------+----------+---------
   bucket                     | text    |           |          | 
   job                        | text    |           |          | 
   audit_id                   | text    |           | not null | 
   stage                      | text    |           | not null | 
   event_verb                 | text    |           | not null | 
   request_uri                | text    |           | not null | 
   operation_id               | text    |           |          | 
   event_level                | text    |           |          | 
   event_stage                | text    |           |          | 
   api_version                | text    |           |          | 
   useragent                  | text    |           |          | 
   test_hit                   | boolean |           |          | 
   conf_test_hit              | boolean |           |          | 
   event_user                 | jsonb   |           |          | 
   object_namespace           | text    |           |          | 
   object_type                | text    |           |          | 
   object_group               | text    |           |          | 
   object_ver                 | text    |           |          | 
   source_ips                 | jsonb   |           |          | 
   annotations                | jsonb   |           |          | 
   request_object             | jsonb   |           |          | 
   response_object            | jsonb   |           |          | 
   response_status            | jsonb   |           |          | 
   stage_timestamp            | text    |           |          | 
   request_received_timestamp | text    |           |          | 
   data                       | jsonb   |           | not null | 
  Indexes:
      "idx_audit_event_bucket" btree (bucket)
      "idx_audit_event_conf_test_hit" btree (conf_test_hit)
      "idx_audit_event_job" btree (job)
      "idx_audit_event_operation_id" btree (operation_id)
      "idx_audit_event_test_hit" btree (test_hit)
  Triggers:
      add_op_id BEFORE INSERT ON audit_event FOR EACH ROW WHEN (new.job = 'live'::text) EXECUTE FUNCTION add_op_id()

  #+end_SRC

** look at what tilt does

  #+begin_src sql-mode :eval never-export :exports both :session none
    select distinct useragent
      from audit_event
     where bucket='apisnoop'
       and useragent not like 'kube%'
       and useragent not like 'coredns%'
       and useragent not like 'kindnetd%'
      -- and useragent like 'live%'
       and operation_id in (
         connectCoreV1PutNodeProxyWithPath,
         connectCoreV1PutNodeProxy,
         connectCoreV1PostNodeProxyWithPath,
         connectCoreV1PostNodeProxy,
         connectCoreV1PatchNodeProxyWithPath,
         connectCoreV1PatchNodeProxy,
         connectCoreV1OptionsNodeProxyWithPath,
         connectCoreV1OptionsNodeProxy,
         connectCoreV1HeadNodeProxyWithPath,
         connectCoreV1HeadNodeProxy,
         connectCoreV1GetNodeProxy,
         connectCoreV1DeleteNodeProxyWithPath,
         connectCoreV1DeleteNodeProxy
         )
        ;
  #+end_src

  #+RESULTS:
  #+begin_SRC example
  ERROR:  column "connectcorev1putnodeproxywithpath" does not exist
  LINE 9:      connectCoreV1PutNodeProxyWithPath,
               ^
  #+end_SRC

** later

List endpoints hit by the test:
#+begin_src sql-mode :exports both :session none
select * from endpoints_hit_by_new_test where useragent like 'live%'; 
#+end_src

Display endpoint coverage change:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select * from projected_change_in_coverage;
  #+end_src

  #+RESULTS:
  #+begin_SRC example
     category    | total_endpoints | old_coverage | new_coverage | change_in_number 
  ---------------+-----------------+--------------+--------------+------------------
   test_coverage |             438 |          183 |          183 |                0
  (1 row)

  #+end_SRC

* Verifying increase in coverage with APISnoop                       :export:
Discover useragents:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select distinct useragent from audit_event where bucket='apisnoop' and useragent not like 'kube%' and useragent not like 'coredns%' and useragent not like 'kindnetd%' and useragent like 'live%';
  #+end_src

List endpoints hit by the test:
#+begin_src sql-mode :exports both :session none
select * from endpoints_hit_by_new_test where useragent like 'live%'; 
#+end_src

Display endpoint coverage change:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select * from projected_change_in_coverage;
  #+end_src

  #+RESULTS:
  #+begin_SRC example
     category    | total_endpoints | old_coverage | new_coverage | change_in_number 
  ---------------+-----------------+--------------+--------------+------------------
   test_coverage |             438 |          183 |          183 |                0
  (1 row)

  #+end_SRC

* Final notes :export:
If a test with these calls gets merged, **test coverage will go up by N points**

This test is also created with the goal of conformance promotion.

-----  
/sig testing  

/sig architecture  

/area conformance  

* *Check it all worked
